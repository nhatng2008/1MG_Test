{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import StructField, IntegerType, StructType, StringType, DateType, DoubleType, TimestampType\n",
    "\n",
    "spark = SparkSession.builder.appName('OneMountGroupTest').getOrCreate()\n",
    "\n",
    "\n",
    "data_file = \"green_tripdata_2013-09.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema():\n",
    "    schema=[StructField('VendorID',IntegerType(),True),\n",
    "           StructField('lpep_pickup_datetime',TimestampType(),True),\n",
    "           StructField('Lpep_dropoff_datetime',TimestampType(),True),\n",
    "           StructField('Store_and_fwd_flag',StringType(),True),\n",
    "           StructField('RateCodeID',IntegerType(),True),\n",
    "           StructField('Pickup_longitude',DoubleType(),True),\n",
    "           StructField('Pickup_latitude',DoubleType(),True),\n",
    "           StructField('Dropoff_longitude',DoubleType(),True),\n",
    "           StructField('Dropoff_latitude',DoubleType(),True),\n",
    "           StructField('Passenger_count',IntegerType(),True),\n",
    "           StructField('Trip_distance',DoubleType(),True),\n",
    "           StructField('Fare_amount',DoubleType(),True),\n",
    "           StructField('Extra',DoubleType(),True),\n",
    "           StructField('MTA_tax',DoubleType(),True),\n",
    "           StructField('Tip_amount',DoubleType(),True),\n",
    "           StructField('Tolls_amount',DoubleType(),True),\n",
    "           StructField('Ehail_fee',DoubleType(),True),\n",
    "           StructField('Total_amount',DoubleType(),True),\n",
    "           StructField('Payment_type',IntegerType(),True),\n",
    "           StructField('Trip_type',IntegerType(),True)\n",
    "    ]\n",
    "    return StructType(fields=schema)\n",
    "\n",
    "\n",
    "\n",
    "# Read data from file and convert the schema\n",
    "df =  spark.read.format(\"csv\").option(\"header\", \"true\").option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\").schema(get_schema()).load(data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[VendorID: int, lpep_pickup_datetime: timestamp, Lpep_dropoff_datetime: timestamp, Store_and_fwd_flag: string, RateCodeID: int, Pickup_longitude: double, Pickup_latitude: double, Dropoff_longitude: double, Dropoff_latitude: double, Passenger_count: int, Trip_distance: double, Fare_amount: double, Extra: double, MTA_tax: double, Tip_amount: double, Tolls_amount: double, Ehail_fee: double, Total_amount: double, Payment_type: int, Trip_type: int]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 1. Create a program that produces a typed parquet file (https://parquet.apache.org/) from this file\n",
    "stored_file = '_data/converted_data.parquet'\n",
    "df.write.save(stored_file)\n",
    "\n",
    "\n",
    "# Read data from Parquet and check type \n",
    "df_from_parquet = spark.read.parquet(stored_file)\n",
    "\n",
    "df_from_parquet.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.Create a derived dataset, from the one created above, using a SQL statement that selects all existing columns and adds these new columns:\n",
    "#* One-Hot encoding for each hour of the day\n",
    "#* One-Hot encoding for each day\tof the week\n",
    "#* Duration in seconds of the trip\n",
    "#* An int encoding to indicate if the pickup or dropoff locations were at JFK airport. (Use a bounding box from the GPS coordinates to determine this). Provide pseudo code if out of time. This column is optional.\n",
    "\n",
    "# We assume the following values are GPS coordinates of JSK Airport, please change it in case you need\n",
    "# JSK Airport Bounding BOX: 10KM from the JSK Airport LAT,LONG (40.5544017, 40.73440170000001, -73.8781194, -73.6981194)\n",
    "jsk_gps = {\n",
    "    \"max_lat\": 40.73440170000001,\n",
    "    \"min_lat\": 40.5544017,\n",
    "    \"max_long\": -73.6981194,\n",
    "    \"min_long\": -73.8781194\n",
    "}\n",
    "\n",
    "\n",
    "def is_at_JSK(lat , long):\n",
    "    return  1 if lat <= jsk_gps['max_lat'] and lat >= jsk_gps['min_lat'] and long <= jsk_gps['max_long'] and long >= jsk_gps['min_long'] else 0\n",
    "\n",
    "spark.udf.register(\"is_at_JSK\", is_at_JSK)\n",
    "\n",
    "\n",
    "# INIT SQL \n",
    "\n",
    "sql_table = \"Trip_Data\"\n",
    "\n",
    "df_from_parquet.createOrReplaceTempView(sql_table)\n",
    "\n",
    "\n",
    "### To generate new cols: \n",
    "### - en_h : One-Hot encoding for each hour of the day\n",
    "### - en_d : One-Hot encoding for each day\tof the week\n",
    "### - duration : Duration in seconds of the trip\n",
    "### - is_at_JFK: An int encoding to indicate if the pickup or dropoff locations were at JFK airport\n",
    "\n",
    "\n",
    "sql_query = \"\"\"\n",
    "select *, \n",
    "       hour(lpep_pickup_datetime) as en_h, \n",
    "       dayofweek(lpep_pickup_datetime) as en_d,\n",
    "       unix_timestamp(Lpep_dropoff_datetime) - unix_timestamp(lpep_pickup_datetime) as duration,\n",
    "       CASE WHEN is_at_JSK(Pickup_latitude, Pickup_longitude) = 1 \n",
    "                 OR is_at_JSK(Dropoff_latitude, Dropoff_longitude) = 1 \n",
    "                     THEN 1\n",
    "                 ELSE 0\n",
    "                 END AS is_JSK\n",
    "from {0}\n",
    "\"\"\".format(sql_table)\n",
    "\n",
    "\n",
    "df_sql = spark.sql(sql_query)\n",
    "\n",
    "# Save new data to Parquet\n",
    "new_stored_file = '_data/new_converted_data.parquet'\n",
    "df_sql.write.save(new_stored_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
